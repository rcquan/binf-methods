---
title: "Computational Methods II"
author: "Ryan Quan (rcq2102)"
date: "January 20, 2015"
output:
  html_document:
    toc: yes
---

# 08 - Association Rules

Just a reminder: the relationship between mutual information and KL-divergence.

$I(X;Y) = \sum\limits_{x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} = D_{KL}(p(x,y)||p(x)p(y))$

Dataset $n x p$ matrix:

* n = nb of objects
* p = nb of features, attributes
    
**Uses**

* Exploratory Data Analysis
* Predicitve Modeling
* Discourse Patterns

## A-priori Algorithm (1994)

[Link](http://nikhilvithlani.blogspot.com/2013/07/apriori-algorithm-for-data-mining-made.html)

* Task: rule pattern discovery
* Scores: is a function of **support** and **confidence**
* Search: breadth-first search and pruning
* Rule: lhs ==> rhs
* Data: Categorical

$I = (i_1...i_m)$ items

$D =$ set of transactions ($n x p$ matrix)

Transaction $T$ is a set of items in where $T$ contains $X$ if $X \leq T$

Association of $X$ ==> $Y$

$X$ is a subset of  $I$, $Y$ is a subset of $I$; $X \cap Y = 0$

### Key Terms

**Confidence** of a rule = c, where c the number of times X and Y are purchased divided by the the number of times X is purchased alone; $\frac{support(lhs, rhs)}{support(lhs)}$

**Lift** of a rule is $\frac{support(lhs)}{support(lhs)support(rhs)}$

**Support** of rule = s, where s is the % of transactions in D containing X.

**Itemset** is a set of items.

**k-itemset** is a set of k items.

**Frequent k-itemset** is a k-item set with high support.

### Algorithm

The naive algorithm would require us to iterate through each itemset, which would require us to enumerate all itemsets ($2^n$).

$C_k$: candidate itemsets of size k

$F_k$; frequent itemsets of size k

$F_1$ = {frequent items}

```
for (k=1; $F_k$ != 0, k++) {
    $C_{k+1}$ = generate candidate ($F_k$)
    for each transaction $T$ in $D$
        increment count of $C_{k+1}$ if C_{k+1} is a subset of $T$
    $F_{k+1}$ = candidates in $C_{k+1}$ with high frequency
return set of $F_k$
}
```

```
for each itemset 
    s ==> (l-s) if s is a non-empty subset of itemset
    compute confidence/lift and prune
```

**Example**

1. Start with transaction records and set minimum support and minimum confidence parameters.
2. Find support for item sets for k = 1; remove from candidate list those with support less than theta.
3. Repeat process for k = 2, 3… (can’t use sets that have already been discarded)
4. Generate full itemset list. Calculate confidence to make candidate rules.
5. Use minimum confidence to prune rules, keeping rules with minconf >= 0.75

# 07 - More Information Theory

## Conditional Entropy

$H(Y|X) = \sum\limits_{x} p(x)H(Y|X=x)$

$H(Y|X) = -\sum\limits_{x}\sum\limits_{y} p(y|x) logp(y|x)$

How much info is still needed to convey about Y if X is known?

$H(X,Y) = H(X) + H(Y|X)$

$H(X_1...X_n) = H(X_1) + ... + H(X_n|X_1...X_{n-1})$

## Cross Entropy

The cross entropy between two probability distributions over the same underlying set of events measures the average number of bits needed to identify an event drawn form the set, if a coding scheme is used that is optimized for an "unnatural" probability distribution q, rather than the "true" distribution p. 

$H(p,q) = E_p[-log q] = H(p) + D_{KL}(p||q)$

## KL Divergence (Relative Entropy)

KL stands for Lullback Leibler.

Let P,Q be two probability distributions over the same sample space $\Omega$.

$D_{KL}(P||Q) = \sum\limits_{x} p(x)log\frac{p(x)q(x)} = E_p(log\frac{p}{q})$

Specifically, the Kullback–Leibler divergence of $Q$ from $P$, denoted $D_{KL}(P‖Q)$, is a measure of the information lost when $Q$ is used to approximate $P$. 

Typically $P$ represents the "true" distribution of data, observations, or a precisely calculated theoretical distribution. The measure $Q$ typically represents a theory, model, description, or approximation of $P$.

$D_{KL}(P||Q) = \sum\limits_{x} p(x)log\frac{p(x)q(x)} = - \sum\limits_{x}p(x)logq(x) + \sum\limits_{x}p(x)log(p(x)$

KL Divergence of Q from P = Cross Entropy(P,Q) - Entropy(P)

Note that $D_{KL}$ is not symmetric!

### Two Uses of KL Divergence

1. How different 2 distributions are from each other?
2. Estimate validity of model compared to truth

**How different 2 distributions are from each other?**

## Gibb's Inequality

$D_{KL}(P||Q) \geq 0$

Equals 0 if $P = Q$.

$x_1: p_1...x_n:p_n$

$x_1:q_1...x_n:q_n$

$D_{KL}(P||Q) = \sum\limits_{i=1}^{n}p_i log\frac{p_i}{q_i}$

However, note that if $p_1 \neq 0$ and $q_i = 0$, the KL Divergence converges to infinity. Thus, we need to account for unseen events by **smoothing.**

## Absolute Discounting

Another method for smoothing.

1. Define $E$ = 0.0001
2. $SP = {a, b, c}, SQ = {a, b, d}, SU = {a, b, c, d}$
3. Smoothed versions of p $\propto$ q

$p_i = p_i-p_c$ if $i \in SP$
$p_i = E$ otherwise

such that $\sum p'_i = 1.

**Estimate validity of model compared to truth**

$D_{KL}(P||Q) = H(P,q) - H(P)$

Cross-entropy is the **perplexity** of a dataset. Goal is to find Q that minimizes the $D_{KL}$ -- Q that yields the lowest cross entropy. 

If $D_{KL}(P||model_1) < D_{KL}(P||model_2)$, model 1 is better!

## Mutual Information

H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)

H(X) = H(X|Y) = H(Y) - H(Y|X) = I(X;Y) = I(Y;X)

![mutualinfo](http://www.quantiki.org/mediawiki/images/d/dd/Classinfo.png)

# 06 - Language Modeling

Link to this slide [here](03_languagemodeling.pdf).
Link to Dan Jurafsky's NLP slides [here](http://web.stanford.edu/~jurafsky/NLPCourseraSlides.html)

## Introduction

The goal of a probabilistic langugae model is to **compute the probability of a sentence or sequence of words.**

We can compute the joint probability of words by relying on the **Chain Rule of Probability.**

$P(x_1,x_2,x_3,...,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1...x_{n-1})$

However, computing joint probabilities is not practical since we can't keep track of all possible histories of each word. We thus use the **Markov Assumption** to approximate each component in the product.

$P(w_i|w_1w_2...w_n) \approx \prod\limits_{i} P(w_i|w_{i-k}...w_{i-1})$

In the simplest case, we have the **unigram model** in which we approximate the joint probability of all words with the product of the the probabilities of each word. **Bigram model** conditions on the previous word.

## Estimating Probabilities

$P(w_i|w_{i-1}) = \frac{c(w_i, w_{i-1})}{c(w_{i-1})}$

Essentially, we are normalizing the counts of the bigrams with the counts of the unigrams. Remember to do everything in log to avoid underflows.

[Google N-Gram Release, August 2006](http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html)

## Evaluation and Perplexity

**Extrinsic** evaluation is the best, but very time consuming. **Intrinsic** evaluation is faster but less accurate. Enter perplexity.

**Perplexity** is the inverse probability of the test set, normalized by the number of words. The best language model is one that best predicts an unseen test site. **Minimizing perplexity is the same as maximizing probability.**

## Basic Smoothing

When we sparse statistics, we can steal probability mass to generalize better. Essentially, we are trying to **prevent zero probabilities.**

### Add-one (Laplace)

Add-one estimation. Pretend we saw each word one more time than we did. Here is an example for bigrams:

$P*(w_n|w_{n-1}) = \frac{c(w_{n-1}, w_n) + 1}{c(w_{n-1}) + V}$

Laplace smoothing is typically used for text classification and in domains where the numbers of zeros aren't too huge. Otherwise, there are better methods, especially for n-grams.

But sometimes it helps to use **less* context. We can condition on less context for contexts you haven't learned about using other methods.

### Backoff

In **backoff**, we use trigrams if we have good evidence - otherwise bigram - otherwise unigram. In **stupid backoff** there is no discounting - just use relative frequencies. Use when you have very large N-grams like the web.

### Interpolation

In **interpolation**, we can mix unigram, bigram, trigram and weight them with $\lambda$. Use cross-validaiton to determine the $\lambda$ that maximizes the probability of the cross-validation set. Most commonly used method is the **extended interpolated kneser-ney**.

### Add-k

Maybe add-one is a little too much. We can get a more general formulation:

$P_{add-k}(w_i|w_{i-1}) = \frac{c(w_{i-1}, w_i) + k}{c(w_{i-1}) + kV}$

### Unigram prior smoothing

$P_{unigramprior}(w_i|w_{i-1}) = \frac{c(w_{i-1}, w_i) + mP(w_i)}{c(w_{i-1}) + m}$

## Advanced Smoothing

Use the count of things we've seen once to help estimate the count of things we've never seen before.

For example, let's say we were talking about fish and mentioned carp, perch, whitefish, salmon. The probability of a new fish species occuring can be estimated using the probability of "things-we-saw-once". Assuming so, how likely is it that the next speciies is "trout"?

### Good-Turing

Good–Turing frequency estimation is a statistical technique for estimating the probability of encountering an object of a hitherto unseen species, given a set of past observations of objects from different species. (In drawing balls from an urn, the 'objects' would be balls and the 'species' would be the distinct colors of the balls (finite but unknown in number).

[Wikipedia](https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation)

### Kneser-Ney

### Witten-Bell

# 05 - Information Theory

## History

The early beginnings of information theory started with lines of communication during the war. People sent messages through what was essentially a "black box" and wanted to know how to decrypt it on the other end with some degree of certainty. Enter information theory.

Seminal paper: [Mathematical Theory of Communication (Shannon)](http://www.greentouch.org/?page=shannons-law-explained)

## Primer

Let's say we want to quantify information content over the channel. Here are some concepts and terms that we want to be familiar with:

* noisy channel, entropy, mutual information
* source coding theorem
    * on average, the number of bits required to represent the message / result of an uncertain event is given by its **entropy**. 
    
## Entropy

### Entropy of an Outcome

The **information content of an outcome** can be defined as:

$H(X) = -log_2 p(x)$

### Entropy of a Random Variable

The **information content of a random variable** can be defined as:

$H(p) = H(x) = -\sum\limits_{x}p(x)log_2p(x)$

This is essentially the expectation of entropy $E(-log_2p(x))$.

**Extreme Cases**

In cases of complete uncertainty, we have a uniform distribution. As such, the entropy will be the following:

$H(X) = -log_2(1/n)$ - very high!

In cases of complete certainty, the entropy will be zero - very low!

### Joint Entropy of Two Random Variables

$H(X,Y) = -\sum\limits_{x,y} p(x,y)log_2p(x,y)$

$H(X,Y) = -\sum\limits_{x} \sum\limits_{y}$

# 04 - Probability Distributions

Given a distribution, we can apply the following properties:

* **sample**. draw a random variable from the distribution such that if we keep sampling, the objects will follow the distribution
* **estimate**. given a set of samples, guess which distribution is underlying the sample.
* **marginalize**. given $p(x,y)$, compute $p(x)$ or $p(y)$
* **condition**. given $p(x,y)$, compute $p(y|x)$ or $p(x|y)$
* **expectation**.
* **variance**.

But often we do not know the underlying probability distribution function. We must estimate using the data and/or make some assumptions about the data.

## Maximum Likelihood Estimation (MLE)

Given dataset D, what is the likelihood of a coin flipping heads?

$\theta^k = argmax P(D|\theta)$

We can use MLE to estimate the model's parameters. Essentially, we are finding the parameter that maximizes the likelihood given the data.

**Example**

Let's say we flipped a coin such that $D = H H T$. What is the probability of flipping heads, $m$?

Since each flip is independent of one another, we can use the following property: 

$$
(m)(m)(1-m) = 1 \\
argmax(m^2 - m^3 -1 = 0) \\
2 - 3m  = 0 \\
m = 2/3
$$

We can use MLE, uniform distribution, etc. to estimate the probability distribution and see which one best fits our data. Note that in MLE, **you can have zero probabilities** if an observation does not appear in the training set. This becomes a problem when using the model to predict on a testing set and **can be corrected via smoothing**, e.g. laplace.

**Issues**

* MLE maximizes the data $D$ and doesn't handle unseen events well.
* MLE doesn't handle uncertainty well, e.g. urn with replacement

### MLE -> Bayesian

MLE with prior $p(\theta)$.

$\theta = argmax p(\theta)p(D|\theta)$

$\theta = argmax \frac{p(\theta|D)}{p(D)}$

Now with the prior included, instead of maximizing data $D$ given the parameter, we are maximizing the parameter given the data.

## Standard Discrete Distributions

### Bernoulli Process

Single experiment with a binary outcome, e.g. a coin toss.

$pr(X = 1) = p$

A Bernoulli process assumes $n$ Bernoulli trials and iid - independentaly identically distributed trials.

From the Bernoulli process, we get a few well-known distributions:

* [Binomial](https://en.wikipedia.org/wiki/Binomial_distribution)

# 03 - More Probability

## Expectation and Variance

Now let's say we have a random variable $X$ that is assigned to a probability distribution $P$ and that $X$ is part of sample space of real numbers.

### Expectation

$E(X) = \sum\limits_{\omega=\Omega} X(\omega)M_p(\omega)$

$E(X) = \int_{\Omega} X dP$

**Properties**

$X_1 \leq X_2; E(X_1) \leq E(X_2)$

$E(aX_1 + bX_2) = aE(X_1) + bE(X_2)$

$E(X_1 X_2)$ does not equal to $E(X_1)E(X_2)$ when $P(X_1)$ and $P(X_2)$
are independent of each other.

**Example**

Let's say we want to know whether or not we should wait to take the express train or get on the local train now. The express train gets to our destination 5 minutes faster.

We have two outcomes:

* wait for the express for 2 minutes - p = 0.75
* wait for the express for 10 minutes - p = 0.25

On average, how long should I wait?

First, we should figure out what our random variable $X$ is. In this case, it is the **time gained taking the express**.

$$
X(2) = 5 - 2 = 3 \\
X(5) = 5 - 10 = -5 \\ \\
E(X) = X(2)P(2) + X(10)P(10) \\
3(0.75) + (-5)(0.25) \\
= 1 minute.
$$

On average, we will gain 1 minute of time taking the express so we should wait.

### Variance

How much does an event, on average, deviate from its mean?

$Var(X) = E((X - E(X)^2))$

$Var(X) = E(X^2) - E(X)^2$

### Co-Variance

How much two random variables change together.

$Cov(X, Y) = E - [(X-E(X))(Y-E(Y))]$

$Cov(X, Y) = E_{xy}(XY) - E(X)E(Y)$

# Lab 1 - Intro to R

## Data Types

* 1-D
* 2-D
* Homogeneous
* Heterogeneous

1-D / Homogeneous - vector  
2-D / Homogeneous - matrix

1-D / Heterogenous - list  
2-D / Heterogenous - data frame

## CRUD

### Create

```{r, eval=FALSE}
## vector
c(1, 2, 3, 4)
vector(length = 100)

## matrix
matrix(data, nrow = n, ncol = m, byrow = FALSE)

## list
list(name = "Ryan", age = 100)

## data frame
data.frame(names = "Ryan", age = 100)
```

### Read (Access)

For vector index, you can use one of the following:

1. numeric
2. named access
3. boolean vectors

For lists, we can do one of the following:

```{r, eval=FALSE}
l$name # name
l[["name"]] # gets you the object stored in that position
l["name"] # gets you the sub-list
```

### Update

```{r, eval=FALSE}
## vectors
c(vec, 1, 2, 3, 4)

## matrices
rbind(mat, row)
cbind(mat, col)

## lists
l[["newfield"]]
```

### Destroy

For vectors and matrices, just sub-select and get a new vector or matrix.

```{r, eval=FALSE}
## lists
l[[NULL]]
```

## Object-Oriented Programming

## Programming Constructs - FUNCTIONS

### S4 Object

1. "Real" object implementation
2. Named records with slots
3. Obj@Slot syntax
4. Dispatch from call tables

### S3 Object

1. List with class attribute
2. Name-based dispatch of generics

### Applying Functions

R is built around vectors so function calls are broadcasted across vectors. 

# 02 - Probability Theory

## Axioms

$$
P(A \cup B) + P(B) - P(A \cap B) \\
P(True) = 1 \\
P(False) = 0
$$

**Example**

$P(A') = 1 - P(A)$  

**Proof**

$P(A \cup A') = P(A) + P(A') - P(A \cap A')$  
$P(A \cup A') = P(A) + P(A') - P(False)$  
$P(A \cup A') = P(A) + P(A')$

## Multi-Valued Random Variable

A can take several disjoint values $v_1...v_k$.

$P(A = v_i \cap A = v_j) = 0$

The events that generate $A = v_1$ are disjoint.

The likelihood of independent events is equivalent to the sum of their likelihoods.

**Example**

$P(A = v_1 \cup...\cup A = v_i) = \sum_{j=1}^i P(A = v_j)$

**Proof**

$$
P(A = v_1 \cup...\cup A = v_i) = P(A = v_1) + P(A = v_2 \cup...\cup A = v_i)
- P(A = v_1 \cap (A = v_2 \cup...\cup A = v_i)) \\
= P(A = v_1) + P(A = v_2) + P(A = v_3 \cup...\cup A = v_i)
$$

## Conditional Event

$P(A|B) = \frac{P(A \cap B)}{P(B)}; P(B \neq 0)$

$P(\omega|B) = \frac{P(\omega \cap B)}{P(B)} = \frac{P(B)}{P(B)} = 1$

where $P(A|B) \geq 0$

Sanity Check: Conditional probability is a probability.

$$
A_1, A_2 disjoint P(A_1 \cup A_2 | B) \\
= \frac{P((A_1 \cup A_2) \cap B)}{P(B)} \\
= \frac{P((A_1 \cap B) \cup (A_2 \cap B))}{P(B)} \\
= \frac{P(A_1 \cap B) + P(A_2 \cap B)}{P(B)} \\
= P(A_1|B) + P(A_2|B)
$$

## Chain Rule

Will be widely used throughout this class.

$P(A \cap B) = P(A|B)P(B)$

$P(A_1 \cap A_2 \cap... A_n) = P(A_1)P(A_2|A_1)...P(A_n|A_1A_2...A_n)$

**Proof**

Use the definition of conditional probability to simplify the equation.

## Total Probability Theorem

Let $A_1...A_n$ disjoint, from a position of $\Omega$ and $P(A_1) > 0$

$P(B) = P(A_1 \cap B) + ... + P(A_n \cap B)$

$P(B) = P(B|A_1)P(A_1) + ... + P(B|A_n)P(A_n)$

The likelihood of event B is equal to the sum of all conditional probabilities in a given space. You can think of it as the weighted average of the conditional events.

## Bayes Rule

$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$

Proof: Use the definition of conditional probability to simplify the equation.

$P(Y|X_1)$ is the likelihood of observing Y if X_1 is present (posterior).
$P(X_1|Y)$ is the likelihood of observing X_1 if Y is present (likelihood).
$P(X_1)$ is the prior probability.

## Independence

$P(A|B) = P(A) <=> A \perp B$

## Conditional Independence

$P(A \cap B | C) = P(A|C)P(B|C)$ where $A \perp B | C$

## Discrete Probability Space

$\omega$: sample space, countable, finite
$M_p$: probability mass function (pmf)

$M_p: \Omega <=> R^+$

$M_p(\omega) = P({|\omega})$

**Properties**

$M_p(\omega) \in [0,1]$

Mass probability function is in the set [0, 1].

$\sum\limits_{\omega} M_p = 1$

The sum of the mass probability functions are equal to 1.

**Example**

Imagine you have 4 elements: A, T, C, G. 

* You have a 1-scanner that reads one element at a time. 
* A k-scanner reads k elements at a time and its scans are independent of one another.
* The odds of picking a blank "_" = $1/9$
* The scan is not partial to any individual element.

Question: (2-scanners) What is the probability of having a non-homogenous scan with 2+ letters ("_" doesn't count). 

Answer: Since the scanners are independent of one another, we can assume independence and say that the probability of a scan picking up A, C, T, or G is the same. Since the scanner is not partial to any particular base, we can use the property above and sum the mass probability functions to equal to 1. 

$$
1/9 + 4m = 1 \\
m = 2/9 \\
Pr(AT) = Pr(A)Pr(T) \\
= 2/9 * 2/9
= 4/81
$$

# 01- Introduction

## Projects

Must submit a letter of intent and do one of the following:

1. Use a dataset provided for you
    * Online Health Community (10 years, 120K users)
        * Sentiment Analysis
        * Topic Modeling
    * Electronic Health Records (26K, messy, ICU clinical)
    * Comparable Corpus of PubMed Abstracts (with controls)
2. Use a dataset that you already have

Some sample research questions:

* Automated Topic Modeling
* Journalism / PubMed Abstracts in Clinical Journals
* Opinions on Complementary and Alternative Medicine in an OHC

## Grading

* 10% - Class Participation
* 25% - Labs (Due Friday 11:59PM)
* 25% - Midterm
* 40% - Project2

## Outline

* Analysis of Large Datasets
* Machine Learning / Data Mining
* How to Inject Knowledge into Datasets

## Examples of Healthcare Analysis

Quality of Care

1. Length of Stay
2. Readmission Rates
    * predict those who are likely to return to the hospital
    